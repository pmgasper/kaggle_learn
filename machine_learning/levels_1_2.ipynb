{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Learn - Machine Learning\n",
    "\n",
    "Working through lessons from https://www.kaggle.com/learn/maching_learning\n",
    "\n",
    "Keeping code from each step for reference. Planning a more streamlined analysis when complete (iowa_housing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "Level 1\n",
    "1. [How Models Work](#part1)<br>\n",
    "2. [Starting Your ML Project](#part2)<br>\n",
    "3. [Selecting and Filtering Data](#part3)<br>\n",
    "4. [Your First Scikit-Learn Model](#part4)<br>\n",
    "5. [Model Validation](#part5)<br>\n",
    "6. [Underfitting, Overfitting and Model Optimization](#part6)<br>\n",
    "7. [Random Forests](#part7)<br>\n",
    "8. [Submitting from Kernel](#part8)<br>\n",
    "\n",
    "Level 2\n",
    "1. [Handling Missing Values](#l2_part1)<br>\n",
    "2. [One Hot Encoding](#l2_part2)<br>\n",
    "3. [XGBoost](#l2_part3)<br>\n",
    "4. [Partial Dependence Plots](#l2_part4)<br>\n",
    "5. [Pipelines](#l2_part5)<br>\n",
    "6. [Cross-Validation](#l2_part6)<br>\n",
    "7. [Data Leakage](#l2_part7)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How Models Work<a class=\"anchor\" id=\"part1\"></a>\n",
    "\n",
    "Course starts basic, but will ramp up quickly.\n",
    "\n",
    "Modeling steps\n",
    "* **fit** or **train** - capture patterns from data to build model\n",
    "* **predict** - apply model to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Starting your ML Project<a class=\"anchor\" id=\"part2\"></a>\n",
    "\n",
    "Iowa housing data available on Kaggle:\n",
    "* https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data<br>\n",
    "\n",
    "**df.describe**\n",
    "* returns summary data for numerical columns <br>\n",
    "    (or non-numerical colums if called on only non-numerical columns)\n",
    "* count - rows with non-missing data in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "Id                                                                      \n",
       "1             60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "2             20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "3             60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "4             70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "5             60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...          ...      ...          ...      ...    ...   ...      ...   \n",
       "1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig    ...     PoolArea PoolQC  Fence  \\\n",
       "Id                                      ...                              \n",
       "1            Lvl    AllPub    Inside    ...            0    NaN    NaN   \n",
       "2            Lvl    AllPub       FR2    ...            0    NaN    NaN   \n",
       "3            Lvl    AllPub    Inside    ...            0    NaN    NaN   \n",
       "4            Lvl    AllPub    Corner    ...            0    NaN    NaN   \n",
       "5            Lvl    AllPub       FR2    ...            0    NaN    NaN   \n",
       "...          ...       ...       ...    ...          ...    ...    ...   \n",
       "1456         Lvl    AllPub    Inside    ...            0    NaN    NaN   \n",
       "1457         Lvl    AllPub    Inside    ...            0    NaN  MnPrv   \n",
       "1458         Lvl    AllPub    Inside    ...            0    NaN  GdPrv   \n",
       "1459         Lvl    AllPub    Inside    ...            0    NaN    NaN   \n",
       "1460         Lvl    AllPub    Inside    ...            0    NaN    NaN   \n",
       "\n",
       "     MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "Id                                                                           \n",
       "1            NaN       0      2    2008        WD         Normal     208500  \n",
       "2            NaN       0      5    2007        WD         Normal     181500  \n",
       "3            NaN       0      9    2008        WD         Normal     223500  \n",
       "4            NaN       0      2    2006        WD        Abnorml     140000  \n",
       "5            NaN       0     12    2008        WD         Normal     250000  \n",
       "...          ...     ...    ...     ...       ...            ...        ...  \n",
       "1456         NaN       0      8    2007        WD         Normal     175000  \n",
       "1457         NaN       0      2    2010        WD         Normal     210000  \n",
       "1458        Shed    2500      5    2010        WD         Normal     266500  \n",
       "1459         NaN       0      4    2010        WD         Normal     142125  \n",
       "1460         NaN       0      6    2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 80 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>46.549315</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>161.319273</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>1474.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond  \\\n",
       "count  1460.000000  1201.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     56.897260    70.049958   10516.828082     6.099315     5.575342   \n",
       "std      42.300571    24.284752    9981.264932     1.382997     1.112799   \n",
       "min      20.000000    21.000000    1300.000000     1.000000     1.000000   \n",
       "25%      20.000000    59.000000    7553.500000     5.000000     5.000000   \n",
       "50%      50.000000    69.000000    9478.500000     6.000000     5.000000   \n",
       "75%      70.000000    80.000000   11601.500000     7.000000     6.000000   \n",
       "max     190.000000   313.000000  215245.000000    10.000000     9.000000   \n",
       "\n",
       "         YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1   BsmtFinSF2  \\\n",
       "count  1460.000000   1460.000000  1452.000000  1460.000000  1460.000000   \n",
       "mean   1971.267808   1984.865753   103.685262   443.639726    46.549315   \n",
       "std      30.202904     20.645407   181.066207   456.098091   161.319273   \n",
       "min    1872.000000   1950.000000     0.000000     0.000000     0.000000   \n",
       "25%    1954.000000   1967.000000     0.000000     0.000000     0.000000   \n",
       "50%    1973.000000   1994.000000     0.000000   383.500000     0.000000   \n",
       "75%    2000.000000   2004.000000   166.000000   712.250000     0.000000   \n",
       "max    2010.000000   2010.000000  1600.000000  5644.000000  1474.000000   \n",
       "\n",
       "           ...         WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count      ...        1460.000000  1460.000000    1460.000000  1460.000000   \n",
       "mean       ...          94.244521    46.660274      21.954110     3.409589   \n",
       "std        ...         125.338794    66.256028      61.119149    29.317331   \n",
       "min        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "25%        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "50%        ...           0.000000    25.000000       0.000000     0.000000   \n",
       "75%        ...         168.000000    68.000000       0.000000     0.000000   \n",
       "max        ...         857.000000   547.000000     552.000000   508.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
       "mean     15.060959     2.758904     43.489041     6.321918  2007.815753   \n",
       "std      55.757415    40.177307    496.123024     2.703626     1.328095   \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000   \n",
       "25%       0.000000     0.000000      0.000000     5.000000  2007.000000   \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000   \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000   \n",
       "max     480.000000   738.000000  15500.000000    12.000000  2010.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    1460.000000  \n",
       "mean   180921.195890  \n",
       "std     79442.502883  \n",
       "min     34900.000000  \n",
       "25%    129975.000000  \n",
       "50%    163000.000000  \n",
       "75%    214000.000000  \n",
       "max    755000.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iowa data for tutorial\n",
    "data = pd.read_csv('data/train.csv', index_col=0)\n",
    "display(data)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Selecting and Filtering Data<a class=\"anchor\" id=\"part3\"></a>\n",
    "\n",
    "For now, selecting variables to explore by intuition. Later will introduce statistical techniques for prioritizing variables.\n",
    "\n",
    "**df.columns** - names of columns in data frame as a pandas Index object (kind of like a Series?)<br> \n",
    "**df.columns.sort_values** - columns sorted alphabetically rather than in order in datatable<br>\n",
    "\n",
    "Selecting columns:\n",
    "* **dot-notation** - like python attribute - df.column<br>\n",
    "* **brackets** - like python dictiorary lookup - df[['column1', 'column2']]<br>\n",
    "* **.loc** and **.iloc** - see pandas tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'Alley', 'BedroomAbvGr',\n",
       "       'BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2',\n",
       "       'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath',\n",
       "       'BsmtQual', 'BsmtUnfSF', 'CentralAir', 'Condition1', 'Condition2',\n",
       "       'Electrical', 'EnclosedPorch', 'ExterCond', 'ExterQual', 'Exterior1st',\n",
       "       'Exterior2nd', 'Fence', 'FireplaceQu', 'Fireplaces', 'Foundation',\n",
       "       'FullBath', 'Functional', 'GarageArea', 'GarageCars', 'GarageCond',\n",
       "       'GarageFinish', 'GarageQual', 'GarageType', 'GarageYrBlt', 'GrLivArea',\n",
       "       'HalfBath', 'Heating', 'HeatingQC', 'HouseStyle', 'KitchenAbvGr',\n",
       "       'KitchenQual', 'LandContour', 'LandSlope', 'LotArea', 'LotConfig',\n",
       "       'LotFrontage', 'LotShape', 'LowQualFinSF', 'MSSubClass', 'MSZoning',\n",
       "       'MasVnrArea', 'MasVnrType', 'MiscFeature', 'MiscVal', 'MoSold',\n",
       "       'Neighborhood', 'OpenPorchSF', 'OverallCond', 'OverallQual',\n",
       "       'PavedDrive', 'PoolArea', 'PoolQC', 'RoofMatl', 'RoofStyle',\n",
       "       'SaleCondition', 'SalePrice', 'SaleType', 'ScreenPorch', 'Street',\n",
       "       'TotRmsAbvGrd', 'TotalBsmtSF', 'Utilities', 'WoodDeckSF', 'YearBuilt',\n",
       "       'YearRemodAdd', 'YrSold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      1460.000000\n",
       "mean     180921.195890\n",
       "std       79442.502883\n",
       "min       34900.000000\n",
       "25%      129975.000000\n",
       "50%      163000.000000\n",
       "75%      214000.000000\n",
       "max      755000.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of prices - target for predictions\n",
    "data.SalePrice.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1162.626712</td>\n",
       "      <td>346.992466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>386.587738</td>\n",
       "      <td>436.528436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>882.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1087.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1391.250000</td>\n",
       "      <td>728.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4692.000000</td>\n",
       "      <td>2065.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1stFlrSF     2ndFlrSF\n",
       "count  1460.000000  1460.000000\n",
       "mean   1162.626712   346.992466\n",
       "std     386.587738   436.528436\n",
       "min     334.000000     0.000000\n",
       "25%     882.000000     0.000000\n",
       "50%    1087.000000     0.000000\n",
       "75%    1391.250000   728.000000\n",
       "max    4692.000000  2065.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of squart foot by floor - potential features\n",
    "data[['1stFlrSF', '2ndFlrSF']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Your First Scikit-Learn Model <a class=\"anchor\" id=\"part4\"></a>\n",
    "\n",
    "Choose a **prediction target** aka **outcome variable** aka **dependent variable** conventionally **y**\n",
    "\n",
    "Choose **predictors** aka **features** aka **independent variables** conventionally **X**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "1       208500\n",
       "2       181500\n",
       "3       223500\n",
       "4       140000\n",
       "5       250000\n",
       "         ...  \n",
       "1456    175000\n",
       "1457    210000\n",
       "1458    266500\n",
       "1459    142125\n",
       "1460    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8450</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9600</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11250</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9550</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14260</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>7917</td>\n",
       "      <td>1999</td>\n",
       "      <td>953</td>\n",
       "      <td>694</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>13175</td>\n",
       "      <td>1978</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>9042</td>\n",
       "      <td>1941</td>\n",
       "      <td>1188</td>\n",
       "      <td>1152</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>9717</td>\n",
       "      <td>1950</td>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>9937</td>\n",
       "      <td>1965</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
       "Id                                                                     \n",
       "1        8450       2003       856       854         2             3   \n",
       "2        9600       1976      1262         0         2             3   \n",
       "3       11250       2001       920       866         2             3   \n",
       "4        9550       1915       961       756         1             3   \n",
       "5       14260       2000      1145      1053         2             4   \n",
       "...       ...        ...       ...       ...       ...           ...   \n",
       "1456     7917       1999       953       694         2             3   \n",
       "1457    13175       1978      2073         0         2             3   \n",
       "1458     9042       1941      1188      1152         2             4   \n",
       "1459     9717       1950      1078         0         1             2   \n",
       "1460     9937       1965      1256         0         1             3   \n",
       "\n",
       "      TotRmsAbvGrd  \n",
       "Id                  \n",
       "1                8  \n",
       "2                6  \n",
       "3                6  \n",
       "4                7  \n",
       "5                9  \n",
       "...            ...  \n",
       "1456             7  \n",
       "1457             7  \n",
       "1458             9  \n",
       "1459             5  \n",
       "1460             6  \n",
       "\n",
       "[1460 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = 'SalePrice'\n",
    "y = data.loc[:, target]\n",
    "\n",
    "features = ['LotArea',\n",
    "            'YearBuilt',\n",
    "            '1stFlrSF',\n",
    "            '2ndFlrSF',\n",
    "            'FullBath',\n",
    "            'BedroomAbvGr',\n",
    "            'TotRmsAbvGrd']\n",
    "X = data.loc[:, features]\n",
    "\n",
    "display(y)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "* Import desired model Class from scikit-learn and initiate an instance. \n",
    "* This initial model was trained on all of data to show that this is a bad\n",
    "  idea (set aside some data for [validation](#part5)).\n",
    "* This initial model uses a single decision tree. More sophisticated models\n",
    "  (such as [random forest](#part 7) are generally preferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "iowa_model = DecisionTreeRegressor()\n",
    "iowa_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "      SalePrice  PredictedPrice\n",
      "Id                             \n",
      "103      118964          118911\n",
      "127      128000          135875\n",
      "146      130000          132500\n",
      "194      130000          132500\n",
      "233       94500          106250\n",
      "...         ...             ...\n",
      "1422     127500          133750\n",
      "1423     136500          134000\n",
      "1432     143750          135875\n",
      "1442     149300          144433\n",
      "1453     145000          146500\n",
      "\n",
      "[24 rows x 2 columns]\n",
      "\n",
      "[137500.]\n",
      "180921.19589041095\n",
      "\n",
      "[60000.]\n",
      "34900\n",
      "\n",
      "[755000.]\n",
      "755000\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predicted_prices = iowa_model.predict(X)\n",
    "print(predicted_prices - y.values) # Most predictions exactly correct\n",
    "print()\n",
    "print(data\n",
    "      .assign(PredictedPrice = predicted_prices.astype('int')) \n",
    "      .loc[:, ['SalePrice', 'PredictedPrice']]\n",
    "      .loc[data.SalePrice != predicted_prices]) # only 24 of 1460 are different\n",
    "print()\n",
    "\n",
    "# 'Average' house\n",
    "#    note .values.reshape only necessiary for single row of features\n",
    "print(iowa_model.predict(data.loc[:, features].mean(axis='rows')\n",
    "                         .values.reshape(1, -1)))\n",
    "print(data.SalePrice.mean())\n",
    "print()\n",
    "\n",
    "# 'Minimal' house\n",
    "print(iowa_model.predict(data.loc[:, features].min(axis='rows')\n",
    "                         .values.reshape(1, -1)))\n",
    "print(data.SalePrice.min())\n",
    "print()\n",
    "\n",
    "# 'Max' house\n",
    "print(iowa_model.predict(data.loc[:, features].max(axis='rows')\n",
    "                         .values.reshape(1, -1)))\n",
    "print(data.SalePrice.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Validation <a class=\"anchor\" id=\"part5\"></a>\n",
    "\n",
    "**Mean Absolute Error** or **MAE**\n",
    "* Average, absolute value of the difference between predicted and actual value\n",
    "* Use **sklearn.metrics.mean_absolute_error**\n",
    "\n",
    "**Validation data**\n",
    "* Setting aside some data before training the model to use for testing the model\n",
    "* Use **sklearn.model_selection.train_test_split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.35433789954339"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using trainging data for validation\n",
    "#   just to illustrate what a bad idea this is\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_prices = iowa_model.predict(X)\n",
    "mean_absolute_error(y, predicted_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32813.75890410959\n"
     ]
    }
   ],
   "source": [
    "# Using separate training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rseed = 0  # Make sure to use this for categorical split as it must be the same\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=rseed)\n",
    "iowa_model = DecisionTreeRegressor()\n",
    "iowa_model.fit(train_X, train_y)\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "\n",
    "print(mean_absolute_error(val_y, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Underfitting, Overfitting and Model Optimization<a class=\"anchor\" id=\"part6\"></a>\n",
    "\n",
    "**overfitting** - model accurately predicts training data, but does not generalize well\n",
    "\n",
    "**underfitting** - model performs poorly even on training data, technically it may generalize well but being consistently poor\n",
    "\n",
    "Decision tree **depth** is the length of the longest path from root to leaf. A shallow tree is prone to underfitting, but a deep tree may be overfit. Calculating the MAE for models trained over a range of depths can help identify an optional trade-off depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  35190\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  27825\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  32662\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  33382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "\n",
    "def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=rseed)\n",
    "    model.fit(predictors_train, targ_train)\n",
    "    preds_val = model.predict(predictors_val)\n",
    "    mae = mean_absolute_error(targ_val, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "# Compare MAE with differing values for max_leaf_nodes parameter\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes,\n",
    "                                                                my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Random Forests  <a class=\"anchor\" id=\"part7\"></a>\n",
    "\n",
    "**Random forests** average predictions of many decision trees for better predictive accuracy. Tend to work well with default parameters, though other models with generally better performance exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24037.48584735812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(train_X, train_y)\n",
    "iowa_predictions = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, iowa_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Submitting from a Kernel  <a class=\"anchor\" id=\"part8\"></a>\n",
    "\n",
    "Information for submitting models to Kaggle competitions.\n",
    "1. Download a .csv with training data<br>\n",
    "2. Use this to train and validate a model<br>\n",
    "3. Download a .csv of testing data (same features as training data, but<br>\n",
    "   no values for the target)<br>\n",
    "4. Use the model to make predictions on the testing data<br>\n",
    "5. Save a two column .csv with the IDs from the test data and the predictions<br>\n",
    "   (no index column)<br>\n",
    "6. Submit the .csv to Kaggle<br>\n",
    "   may have to do this from a Kaggle Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n",
    "# you could use any filename. We choose submission here\n",
    "# my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Handling Missing Values <a class=\"anchor\" id=\"l2_part1\"></a>\n",
    "\n",
    "Potential solutions:\n",
    "1. Drop the columns with NaN\n",
    "   * usually not the best solution, but for mostly NaN columns may make sense\n",
    "   * may throw out useful features\n",
    "   * problems if test set has NaN in other columns\n",
    "   * dropping rows with NaN is even more dubious, may introduce sampling bias<br>\n",
    "   <br> \n",
    "2. Imputation\n",
    "   * Fill the missing value with some number\n",
    "   * **sklearn.preprocessing.Imputer**\n",
    "   * The default is to use the column mean. More sophisticated methods exist but\n",
    "     are generally no better<br>\n",
    "     <br>\n",
    "3. Imputation plus - add a booean 'wasNaN' category for each imputed column\n",
    "   * May be meaningful if NaNs are systematically above or below their column means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolQC          1453\n",
      "MiscFeature     1406\n",
      "Alley           1369\n",
      "Fence           1179\n",
      "FireplaceQu      690\n",
      "LotFrontage      259\n",
      "GarageYrBlt       81\n",
      "GarageType        81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "BsmtFinType2      38\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtCond          37\n",
      "BsmtQual          37\n",
      "MasVnrArea         8\n",
      "MasVnrType         8\n",
      "Electrical         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find columns with NaNs\n",
    "nan_count_by_column = (data.isnull().sum().loc[data.isnull().sum() > 0]\n",
    "                      .sort_values(ascending=False))\n",
    "\n",
    "with pd.option_context('max_rows', 20):\n",
    "    print(nan_count_by_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all numeric features from Iowa data\n",
    "#   Ignoring catagorical data types for now for simplicity\n",
    "#     will add these in next section (level2 - part2)\n",
    "\n",
    "X_num = (data.drop(['SalePrice'], axis='columns')\n",
    "             .select_dtypes(exclude=['object']))\n",
    "y = data.SalePrice\n",
    "\n",
    "X_train_num, X_test_num, y_train, y_test = train_test_split(X_num, y, \n",
    "                                                            random_state=rseed)\n",
    "\n",
    "# Function for training and evaluting each method for each NaN treatment\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GarageYrBlt', 'LotFrontage', 'MasVnrArea']\n",
      "[]\n",
      "[]\n",
      "\n",
      "Mean Absolute Error from dropping columns with Missing Values: 18528\n"
     ]
    }
   ],
   "source": [
    "# Solution 1 - drop columns with NaNs\n",
    "\n",
    "# df.dropna(axis=1) drops all columns with NaNs, but doesn't store which columns\n",
    "# instead save list of columns to drop\n",
    "cols_with_nan = [col for col in X_train_num.columns \n",
    "                 if X_train_num[col].isnull().any()]\n",
    "print(sorted(cols_with_nan))\n",
    "X_train_reduced = X_train_num.drop(cols_with_nan, axis='columns')\n",
    "X_test_reduced = X_test_num.drop(cols_with_nan, axis='columns')\n",
    "\n",
    "# nan_count for training data should be as empty list now    \n",
    "cols_with_nan_train = [col for col in X_train_reduced.columns \n",
    "                       if X_train_reduced[col].isnull().any()]\n",
    "print(sorted(cols_with_nan_train))\n",
    "\n",
    "# check if test data is also free of NaNs after dropping columns\n",
    "cols_with_nan_test = [col for col in X_test_reduced.columns \n",
    "                      if X_test_reduced[col].isnull().any()]\n",
    "print(sorted(cols_with_nan_test))\n",
    "\n",
    "# train and test dropped NaN solution\n",
    "mae = score_dataset(X_train_reduced, X_test_reduced, y_train, y_test)\n",
    "print()\n",
    "print('Mean Absolute Error from dropping columns with Missing Values: {0:.0f}'\n",
    "     .format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from imputing Missing Values: 19298\n"
     ]
    }
   ],
   "source": [
    "# Solution 2 - Imputing\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# perform imputations\n",
    "my_imputer = Imputer()\n",
    "X_train_imputed = my_imputer.fit_transform(X_train_num)\n",
    "X_test_imputed = my_imputer.transform(X_test_num)\n",
    "    \n",
    "# train and test with imputed data\n",
    "mae = score_dataset(X_train_imputed, X_test_imputed, y_train, y_test)\n",
    "print('Mean Absolute Error from imputing Missing Values: {0:.0f}'.format(mae)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LotFrontage_was_missing', 'MasVnrArea_was_missing',\n",
      "       'GarageYrBlt_was_missing'],\n",
      "      dtype='object')\n",
      "\n",
      "Mean Absolute Error from imputing plus imputed categories: 19195\n"
     ]
    }
   ],
   "source": [
    "# Solution 3 - Imputing plus adding 'wasNaN' as a feature per imputed column\n",
    "\n",
    "# Add columns of boolean values for 'was missing' imputed columns\n",
    "X_train_plus = X_train_num.copy()\n",
    "X_test_plus = X_test_num.copy()\n",
    "cols_with_nan = [col for col in X_train_num.columns \n",
    "                 if X_train_num[col].isnull().any()]\n",
    "for col in cols_with_nan:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_test_plus[col + '_was_missing'] = X_test_plus[col].isnull()\n",
    "    \n",
    "# List of added columns\n",
    "print(X_train_plus.columns[X_train_plus.columns.str.contains('_was_missing')])\n",
    "print()\n",
    "\n",
    "# Perform imputations\n",
    "my_imputer = Imputer()\n",
    "X_train_imputed_plus = my_imputer.fit_transform(X_train_plus)\n",
    "X_test_imputed_plus = my_imputer.transform(X_test_plus)\n",
    "    \n",
    "# train and test with imputed data\n",
    "mae = score_dataset(X_train_imputed_plus, X_test_imputed_plus, y_train, y_test)\n",
    "print('Mean Absolute Error from imputing plus imputed categories: '\n",
    "      '{0:.0f}'.format(mae))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using Categorical Data with One Hot Encoding<a class=\"anchor\" id=\"l2_part2\"></a>\n",
    "\n",
    "**One-Hot Encoding:**\n",
    "* Standard approach for handling categorical data \n",
    "* Generally works well for variables that don't take on a large number (roughly 15) of values.\n",
    "  One way to address categories with more values is scikit-learns FeatureHasher\n",
    "* Introduces new binary columns for the presence or absence of each value\n",
    "* Use **pd.get_dummies** to one hot encode features\n",
    "* Use **df.align** to ensure common columns between training and test sets\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "RL         1151\n",
      "RM          218\n",
      "FV           65\n",
      "RH           16\n",
      "C (all)      10\n",
      "Name: MSZoning, dtype: int64\n",
      "\n",
      "Street\n",
      "Pave    1454\n",
      "Grvl       6\n",
      "Name: Street, dtype: int64\n",
      "\n",
      "Alley\n",
      "NaN     1369\n",
      "Grvl      50\n",
      "Pave      41\n",
      "Name: Alley, dtype: int64\n",
      "\n",
      "LotShape\n",
      "Reg    925\n",
      "IR1    484\n",
      "IR2     41\n",
      "IR3     10\n",
      "Name: LotShape, dtype: int64\n",
      "\n",
      "LandContour\n",
      "Lvl    1311\n",
      "Bnk      63\n",
      "HLS      50\n",
      "Low      36\n",
      "Name: LandContour, dtype: int64\n",
      "\n",
      "Utilities\n",
      "AllPub    1459\n",
      "NoSeWa       1\n",
      "Name: Utilities, dtype: int64\n",
      "\n",
      "LotConfig\n",
      "Inside     1052\n",
      "Corner      263\n",
      "CulDSac      94\n",
      "FR2          47\n",
      "FR3           4\n",
      "Name: LotConfig, dtype: int64\n",
      "\n",
      "LandSlope\n",
      "Gtl    1382\n",
      "Mod      65\n",
      "Sev      13\n",
      "Name: LandSlope, dtype: int64\n",
      "\n",
      "Neighborhood\n",
      "NAmes      225\n",
      "CollgCr    150\n",
      "OldTown    113\n",
      "Edwards    100\n",
      "Somerst     86\n",
      "          ... \n",
      "Blmngtn     17\n",
      "BrDale      16\n",
      "Veenker     11\n",
      "NPkVill      9\n",
      "Blueste      2\n",
      "Name: Neighborhood, Length: 25, dtype: int64\n",
      "\n",
      "Condition1\n",
      "Norm      1260\n",
      "Feedr       81\n",
      "Artery      48\n",
      "RRAn        26\n",
      "PosN        19\n",
      "RRAe        11\n",
      "PosA         8\n",
      "RRNn         5\n",
      "RRNe         2\n",
      "Name: Condition1, dtype: int64\n",
      "\n",
      "Condition2\n",
      "Norm      1445\n",
      "Feedr        6\n",
      "Artery       2\n",
      "PosN         2\n",
      "RRNn         2\n",
      "RRAn         1\n",
      "PosA         1\n",
      "RRAe         1\n",
      "Name: Condition2, dtype: int64\n",
      "\n",
      "BldgType\n",
      "1Fam      1220\n",
      "TwnhsE     114\n",
      "Duplex      52\n",
      "Twnhs       43\n",
      "2fmCon      31\n",
      "Name: BldgType, dtype: int64\n",
      "\n",
      "HouseStyle\n",
      "1Story    726\n",
      "2Story    445\n",
      "1.5Fin    154\n",
      "SLvl       65\n",
      "SFoyer     37\n",
      "1.5Unf     14\n",
      "2.5Unf     11\n",
      "2.5Fin      8\n",
      "Name: HouseStyle, dtype: int64\n",
      "\n",
      "RoofStyle\n",
      "Gable      1141\n",
      "Hip         286\n",
      "Flat         13\n",
      "Gambrel      11\n",
      "Mansard       7\n",
      "Shed          2\n",
      "Name: RoofStyle, dtype: int64\n",
      "\n",
      "RoofMatl\n",
      "CompShg    1434\n",
      "Tar&Grv      11\n",
      "WdShngl       6\n",
      "WdShake       5\n",
      "Metal         1\n",
      "ClyTile       1\n",
      "Membran       1\n",
      "Roll          1\n",
      "Name: RoofMatl, dtype: int64\n",
      "\n",
      "Exterior1st\n",
      "VinylSd    515\n",
      "HdBoard    222\n",
      "MetalSd    220\n",
      "Wd Sdng    206\n",
      "Plywood    108\n",
      "          ... \n",
      "Stone        2\n",
      "BrkComm      2\n",
      "CBlock       1\n",
      "AsphShn      1\n",
      "ImStucc      1\n",
      "Name: Exterior1st, Length: 15, dtype: int64\n",
      "\n",
      "Exterior2nd\n",
      "VinylSd    504\n",
      "MetalSd    214\n",
      "HdBoard    207\n",
      "Wd Sdng    197\n",
      "Plywood    142\n",
      "          ... \n",
      "Brk Cmn      7\n",
      "Stone        5\n",
      "AsphShn      3\n",
      "CBlock       1\n",
      "Other        1\n",
      "Name: Exterior2nd, Length: 16, dtype: int64\n",
      "\n",
      "MasVnrType\n",
      "None       864\n",
      "BrkFace    445\n",
      "Stone      128\n",
      "BrkCmn      15\n",
      "NaN          8\n",
      "Name: MasVnrType, dtype: int64\n",
      "\n",
      "ExterQual\n",
      "TA    906\n",
      "Gd    488\n",
      "Ex     52\n",
      "Fa     14\n",
      "Name: ExterQual, dtype: int64\n",
      "\n",
      "ExterCond\n",
      "TA    1282\n",
      "Gd     146\n",
      "Fa      28\n",
      "Ex       3\n",
      "Po       1\n",
      "Name: ExterCond, dtype: int64\n",
      "\n",
      "Foundation\n",
      "PConc     647\n",
      "CBlock    634\n",
      "BrkTil    146\n",
      "Slab       24\n",
      "Stone       6\n",
      "Wood        3\n",
      "Name: Foundation, dtype: int64\n",
      "\n",
      "BsmtQual\n",
      "TA     649\n",
      "Gd     618\n",
      "Ex     121\n",
      "NaN     37\n",
      "Fa      35\n",
      "Name: BsmtQual, dtype: int64\n",
      "\n",
      "BsmtCond\n",
      "TA     1311\n",
      "Gd       65\n",
      "Fa       45\n",
      "NaN      37\n",
      "Po        2\n",
      "Name: BsmtCond, dtype: int64\n",
      "\n",
      "BsmtExposure\n",
      "No     953\n",
      "Av     221\n",
      "Gd     134\n",
      "Mn     114\n",
      "NaN     38\n",
      "Name: BsmtExposure, dtype: int64\n",
      "\n",
      "BsmtFinType1\n",
      "Unf    430\n",
      "GLQ    418\n",
      "ALQ    220\n",
      "BLQ    148\n",
      "Rec    133\n",
      "LwQ     74\n",
      "NaN     37\n",
      "Name: BsmtFinType1, dtype: int64\n",
      "\n",
      "BsmtFinType2\n",
      "Unf    1256\n",
      "Rec      54\n",
      "LwQ      46\n",
      "NaN      38\n",
      "BLQ      33\n",
      "ALQ      19\n",
      "GLQ      14\n",
      "Name: BsmtFinType2, dtype: int64\n",
      "\n",
      "Heating\n",
      "GasA     1428\n",
      "GasW       18\n",
      "Grav        7\n",
      "Wall        4\n",
      "OthW        2\n",
      "Floor       1\n",
      "Name: Heating, dtype: int64\n",
      "\n",
      "HeatingQC\n",
      "Ex    741\n",
      "TA    428\n",
      "Gd    241\n",
      "Fa     49\n",
      "Po      1\n",
      "Name: HeatingQC, dtype: int64\n",
      "\n",
      "CentralAir\n",
      "Y    1365\n",
      "N      95\n",
      "Name: CentralAir, dtype: int64\n",
      "\n",
      "Electrical\n",
      "SBrkr    1334\n",
      "FuseA      94\n",
      "FuseF      27\n",
      "FuseP       3\n",
      "Mix         1\n",
      "NaN         1\n",
      "Name: Electrical, dtype: int64\n",
      "\n",
      "KitchenQual\n",
      "TA    735\n",
      "Gd    586\n",
      "Ex    100\n",
      "Fa     39\n",
      "Name: KitchenQual, dtype: int64\n",
      "\n",
      "Functional\n",
      "Typ     1360\n",
      "Min2      34\n",
      "Min1      31\n",
      "Mod       15\n",
      "Maj1      14\n",
      "Maj2       5\n",
      "Sev        1\n",
      "Name: Functional, dtype: int64\n",
      "\n",
      "FireplaceQu\n",
      "NaN    690\n",
      "Gd     380\n",
      "TA     313\n",
      "Fa      33\n",
      "Ex      24\n",
      "Po      20\n",
      "Name: FireplaceQu, dtype: int64\n",
      "\n",
      "GarageType\n",
      "Attchd     870\n",
      "Detchd     387\n",
      "BuiltIn     88\n",
      "NaN         81\n",
      "Basment     19\n",
      "CarPort      9\n",
      "2Types       6\n",
      "Name: GarageType, dtype: int64\n",
      "\n",
      "GarageFinish\n",
      "Unf    605\n",
      "RFn    422\n",
      "Fin    352\n",
      "NaN     81\n",
      "Name: GarageFinish, dtype: int64\n",
      "\n",
      "GarageQual\n",
      "TA     1311\n",
      "NaN      81\n",
      "Fa       48\n",
      "Gd       14\n",
      "Ex        3\n",
      "Po        3\n",
      "Name: GarageQual, dtype: int64\n",
      "\n",
      "GarageCond\n",
      "TA     1326\n",
      "NaN      81\n",
      "Fa       35\n",
      "Gd        9\n",
      "Po        7\n",
      "Ex        2\n",
      "Name: GarageCond, dtype: int64\n",
      "\n",
      "PavedDrive\n",
      "Y    1340\n",
      "N      90\n",
      "P      30\n",
      "Name: PavedDrive, dtype: int64\n",
      "\n",
      "PoolQC\n",
      "NaN    1453\n",
      "Gd        3\n",
      "Ex        2\n",
      "Fa        2\n",
      "Name: PoolQC, dtype: int64\n",
      "\n",
      "Fence\n",
      "NaN      1179\n",
      "MnPrv     157\n",
      "GdPrv      59\n",
      "GdWo       54\n",
      "MnWw       11\n",
      "Name: Fence, dtype: int64\n",
      "\n",
      "MiscFeature\n",
      "NaN     1406\n",
      "Shed      49\n",
      "Gar2       2\n",
      "Othr       2\n",
      "TenC       1\n",
      "Name: MiscFeature, dtype: int64\n",
      "\n",
      "SaleType\n",
      "WD       1267\n",
      "New       122\n",
      "COD        43\n",
      "ConLD       9\n",
      "ConLw       5\n",
      "ConLI       5\n",
      "CWD         4\n",
      "Oth         3\n",
      "Con         2\n",
      "Name: SaleType, dtype: int64\n",
      "\n",
      "SaleCondition\n",
      "Normal     1198\n",
      "Partial     125\n",
      "Abnorml     101\n",
      "Family       20\n",
      "Alloca       12\n",
      "AdjLand       4\n",
      "Name: SaleCondition, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View counts of values in categorical columns\n",
    "for col in data.columns[data.dtypes == 'object']:\n",
    "    print(col)\n",
    "    print(data[col].value_counts(dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neighborhood', 'Exterior1st', 'Exterior2nd']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flag columns that take greater than 15 different values\n",
    "# should also flag low counts?\n",
    "cat_15plus = [col for col in data.columns[data.dtypes == 'object']\n",
    "              if len(data[col].value_counts(dropna=False)) >= 15]\n",
    "cat_15plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical features\n",
    "y = data.loc[:, 'SalePrice']\n",
    "\n",
    "# Using all catagorical features except those that take 15 or more values\n",
    "#   eventually checkout sklearn FeatureHasher for adding these as well\n",
    "#   may also want to checkout sklearn LabelEncoder \n",
    "X_cat = (data\n",
    "     .drop(['SalePrice'], axis='columns')  # drop targets\n",
    "     .select_dtypes(include=['object']) # drop numerical data\n",
    "     .drop(cat_15plus, axis='columns')  # drop >15 value catagories\n",
    "    )\n",
    "\n",
    "# Alternatively - being more selective with features\n",
    "#  still using intuitive features\n",
    "#cat_features = ['LotConfig', 'Neighborhood', 'Condition1', 'BldgType', \n",
    "#                'HouseStyle', 'Exterior1st', 'Foundation', 'GarageType' ]\n",
    "#X_cat = data.loc[:, cat_features]\n",
    "\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y, \n",
    "                                                                random_state=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform one-hot-encoding\n",
    "X_train_encoded = pd.get_dummies(X_train_cat)\n",
    "X_test_encoded = pd.get_dummies(X_test_cat)\n",
    "\n",
    "# Align columns - ensure X_train and X_test have the same columns in order\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded,\n",
    "                                                        join='inner', \n",
    "                                                        axis='columns')\n",
    "\n",
    "# Check that encoding alignment worked\n",
    "(X_train_encoded.columns == X_test_encoded.columns).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 209)\n"
     ]
    }
   ],
   "source": [
    "# Merge catageorical and numerical features\n",
    "X_train_imputed_plus_df = pd.DataFrame(X_train_imputed_plus, \n",
    "                                  columns=X_train_plus.columns, \n",
    "                                  index=X_train_plus.index)\n",
    "\n",
    "X_train_final = X_train_imputed_plus_df.merge(X_train_encoded, \n",
    "                                           right_index=True, left_index=True)\n",
    "\n",
    "X_test_imputed_plus_df = pd.DataFrame(X_test_imputed_plus, \n",
    "                                  columns=X_test_plus.columns, \n",
    "                                  index=X_test_plus.index)\n",
    "\n",
    "X_test_final = X_test_imputed_plus_df.merge(X_test_encoded, \n",
    "                                           right_index=True, left_index=True)\n",
    "\n",
    "print(X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 80)\n",
      "\n",
      "(1095, 39)\n",
      "(1095, 170)\n",
      "(1095, 209)\n",
      "\n",
      "(365, 36)\n",
      "(365, 170)\n",
      "(365, 209)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(data.shape)\n",
    "print()\n",
    "print(X_train_imputed_plus.shape)\n",
    "print(X_train_encoded.shape)\n",
    "print(X_train_final.shape)\n",
    "print()\n",
    "print(X_test_imputed.shape)\n",
    "print(X_test_encoded.shape)\n",
    "print(X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 18072\n"
     ]
    }
   ],
   "source": [
    "# train and test with one hot encoded categorical data and imputed plus data\n",
    "mae = score_dataset(X_train_final, X_test_final, y_train, y_test)\n",
    "print('Mean Absolute Error '\n",
    "      '{0:.0f}'.format(mae))\n",
    "\n",
    "# Not seeing expected improvement. Tutorial saying to expect improvement \"if\n",
    "# you chose the right variables\". Might be overfitting. Try more carefully\n",
    "# selecting features. Check weight of features in model? Maybe employ \n",
    "# regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Learning to Use XGBoost <a class=\"anchor\" id=\"l2_part3\"></a>\n",
    "\n",
    "**XGBoost** \n",
    "* A leading model for standard tabular data (Pandas DataFrames, as opposed to more exotic types like images and videos)<br><br>\n",
    "* An implementation of the **Gradient Boosted Decision Trees** algorithm<br>\n",
    "  + Cycles that repeatedly build new models and combine them into an ensemble model<br>\n",
    "  + Each new model targets errors of the previous ensemble model<br><br>\n",
    "* Has multiple, important **tuning parameters**\n",
    "  + **n_estimators** - number of cycles\n",
    "    * Typical values range from 100-1000 and depend on the learning rate\n",
    "    * Too low can underfit and too high can overfit<br><br>\n",
    "  + **early_stopping_rounds** - stop iterating when the validation score stops improving\n",
    "    * Set n_estimators high and let model stop itself\n",
    "    * Uses eval_set a list of (X, y) pairs for validation<br><br> \n",
    "  + **learning_rate** aka **eta** ($\\eta$) -  fraction of each new model to add to the ensemble\n",
    "    * Effectively a step size toward the most accurate ensemble model\n",
    "    * Smaller learning rates reduce overfitting and result in more accurate models\n",
    "    * However, smaller learning rates require higher n_estimators and so are more computational expensive<br><br>\n",
    "  + **n_jobs** - supports parallelization for larger data sets<br><br>\n",
    "  + more parameters to learn, but these are typically most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 16776\n"
     ]
    }
   ],
   "source": [
    "# Starting from X_train_final, X_test_final, y_train, y_test\n",
    "#   (all categorical one hot encoded except features that take 15 or more values\n",
    "#    and all numeric, with imputations plus boolean was imputed columns)\n",
    "\n",
    "# xgboost lives in it's own module, but functions similarly to a sklearn-module\n",
    "# sklearn also has gradient boosting regressors, but apparently they're not \n",
    "# quite as good.\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xg_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "xg_model.fit(X_train_final, y_train, verbose=False)\n",
    "\n",
    "predictions = xg_model.predict(X_test_final)\n",
    "mae = mean_absolute_error(predictions, y_test)\n",
    "print(\"Mean Absolute Error: {0:.0f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error with n_estimators =   50: 17505\n",
      "Mean Absolute Error with n_estimators =   75: 17099\n",
      "Mean Absolute Error with n_estimators =  100: 16776\n",
      "Mean Absolute Error with n_estimators =  125: 16611\n",
      "Mean Absolute Error with n_estimators =  150: 16460\n",
      "Mean Absolute Error with n_estimators =  175: 16393\n",
      "Mean Absolute Error with n_estimators =  200: 16344\n",
      "Mean Absolute Error with n_estimators =  500: 16015\n",
      "Mean Absolute Error with n_estimators = 1000: 15929\n",
      "\n",
      "Mean Absolute Error with early stopping best iteration = 119 was: 16611\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with n_estimators parameter\n",
    "# for n_est in [100, 300, 500, 700, 900, 1100]:\n",
    "# for n_est in [119, 120, 121, 122, 123, 124]:\n",
    "#for n_est in [700, 750, 800]:\n",
    "for n_est in [50, 75, 100, 125, 150, 175, 200, 500, 1000]:\n",
    "    xg_model = XGBRegressor(n_estimators=n_est)\n",
    "    xg_model.fit(X_train_final, y_train, verbose=False)\n",
    "    predictions = xg_model.predict(X_test_final)\n",
    "    mae = mean_absolute_error(predictions, y_test)\n",
    "    print(\"Mean Absolute Error with n_estimators = {0:4d}: {1:.0f}\"\n",
    "          .format(n_est, mae))\n",
    "\n",
    "# Or use early_stopping to find best n_estimators\n",
    "xg_model = XGBRegressor(n_estimators=1000)\n",
    "xg_model.fit(X_train_final, y_train, early_stopping_rounds=5, verbose=False,\n",
    "                                     eval_set=[(X_test_final, y_test)]) \n",
    "predictions = xg_model.predict(X_test_final)\n",
    "mae = mean_absolute_error(predictions, y_test)\n",
    "print()\n",
    "print(\"Mean Absolute Error with early stopping best iteration = {1:.0f} \"\n",
    "      \"was: {0:.0f}\".format(mae, xg_model.best_iteration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping seems to be finding at minimum much faster exploring???\n",
    "# improvement after n_estimators=119 marginal / random / overfitting???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error with early stopping best iteration = 119 was: 16611 at learning rate 0.1\n",
      "Mean Absolute Error with early stopping best iteration = 137 was: 16790 at learning rate 0.05\n",
      "Mean Absolute Error with early stopping best iteration = 474 was: 17552 at learning rate 0.01\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "lrs = [0.100, 0.050, 0.010]\n",
    "\n",
    "for lr in lrs:\n",
    "    xg_model = XGBRegressor(n_estimators=1000, learning_rate=lr)\n",
    "    xg_model.fit(X_train_final, y_train, early_stopping_rounds=5, verbose=False,\n",
    "                                         eval_set=[(X_test_final, y_test)]) \n",
    "    predictions = xg_model.predict(X_test_final)\n",
    "    mae = mean_absolute_error(predictions, y_test)\n",
    "    print(\"Mean Absolute Error with early stopping best iteration = {1:.0f} \"\n",
    "          \"was: {0:.0f} at learning rate {2}\"\n",
    "          .format(mae, xg_model.best_iteration, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little concerning that lower learning_rate gives higher mae.\n",
    "#  maybe higher values are overfitting to test data \n",
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4:  Partial Dependence Plots<a class=\"anchor\" id=\"l2_part4\"></a>\n",
    "\n",
    "**Partial Dependence Plots**\n",
    "* Show how specific features affect model predictions, marginalizing over all other features\n",
    "* Can be interpreted similarly to regression coefficients, but\n",
    "  + can capture more complex patterns\n",
    "  + and can be used with any model\n",
    "* sklearn **plot_partial_dependence** quick plots directly from data\n",
    "* sklearn **partial_dependence** returns plot data for nicer visualization with other modules\n",
    "* Adjusting **grid_resolution** can help smooth out noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for gradient boost model = 16923.427135955415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting model for partial dependence plots\n",
    "#  It appears sklearn.ensemble.plot_partial_dependence only supports a\n",
    "#  BaseGradientBoosting regressor. For now retraining the model. Would be good\n",
    "#  to find or code partial dependece for xgboost and other models at some point.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr_model = GradientBoostingRegressor()\n",
    "gbr_model.fit(X_train_final, y_train)\n",
    "predictions = gbr_model.predict(X_test_final)\n",
    "mae = mean_absolute_error(predictions, y_test)\n",
    "print(\"Mean Absolute Error for gradient boost model = {0}\".format(mae))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyYAAADPCAYAAAAAlMXlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/X/03kwpjW2BTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(864x288)\n",
      "<class 'matplotlib.figure.Figure'>\n"
     ]
    }
   ],
   "source": [
    "# Partial dependence plots\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select target features for partial dependence\n",
    "pd_features = ['1stFlrSF', '2ndFlrSF', 'YearBuilt']\n",
    "# Get column numbers of corresponding plots\n",
    "features = [X_train_final.columns.get_loc(feature) for feature in pd_features]\n",
    "\n",
    "# Plot\n",
    "pd_plot = plot_partial_dependence(\n",
    "                    gbr_model,\n",
    "                    features=features, # column numbers of plots we want to show\n",
    "                    X=X_train_final,  # raw predictors data.\n",
    "                    feature_names=X_train_final.columns.values, # labels on graphs\n",
    "                    grid_resolution=10,  # number of values to plot on x axis\n",
    "                    )\n",
    "pd_plot[0].set_size_inches(12, 4)\n",
    "plt.show()\n",
    "\n",
    "print(pd_plot[0])\n",
    "print(type(pd_plot[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for gradient boost model = 21769.906003036485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Partial dependence plots - take2 \n",
    "#   Simpler model, less features, used to figure out plot_partial_dependence\n",
    "#   in previous take\n",
    "\n",
    "# Features\n",
    "target = 'SalePrice'\n",
    "y = data.loc[:, target]\n",
    "\n",
    "features = ['LotArea',\n",
    "            'YearBuilt',\n",
    "            '1stFlrSF',\n",
    "            '2ndFlrSF',\n",
    "            'FullBath',\n",
    "            'BedroomAbvGr',\n",
    "            'TotRmsAbvGrd']\n",
    "X = data.loc[:, features]\n",
    "\n",
    "# Imputation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rseed)\n",
    "imputer = Imputer()\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Model\n",
    "gbr_model2 = GradientBoostingRegressor()\n",
    "gbr_model2.fit(X_train_imputed, y_train)\n",
    "predictions = gbr_model2.predict(X_test_imputed)\n",
    "mae = mean_absolute_error(predictions, y_test)\n",
    "print(\"Mean Absolute Error for gradient boost model = {0}\".format(mae))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAADPCAYAAAD4dD4SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiEb7D+uL/jVwrHqYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partial dependence plots\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select target features for partial dependence\n",
    "pd_features = ['1stFlrSF', '2ndFlrSF', 'YearBuilt']\n",
    "# Get column numbers of corresponding plots\n",
    "features = [X.columns.get_loc(feature) for feature in pd_features]\n",
    "\n",
    "# Plot\n",
    "pd_plot = plot_partial_dependence(\n",
    "                    gbr_model2,\n",
    "                    features=features, # column numbers of plots we want to show\n",
    "                    X=X_train_imputed,  # raw predictors data.\n",
    "                    feature_names=X.columns.values, # labels on graphs\n",
    "                    grid_resolution=10,  # number of values to plot on x axis\n",
    "                    )\n",
    "pd_plot[0].set_size_inches(12, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Pipelines<a class=\"anchor\" id=\"l2_part5\"></a>\n",
    "\n",
    "**Pipelines**\n",
    "* Improve code by bundling preprocessing and modeling\n",
    "* Take any number of transformers, followed by a single model\n",
    "  + **transformers** .fit then .transform (or .fittransform) - use for preprocessing\n",
    "  + **models** .fit then .predict - used for predictions\n",
    "* The returned output of each step is the input for next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for gradient boost model = 21859.414801182724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple pipeline\n",
    "#   Using data from simple example (take two) in partial developement plots \n",
    "#   X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(Imputer(), GradientBoostingRegressor())\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(predictions, y_test)\n",
    "print(\"Mean Absolute Error for gradient boost model = {0}\".format(mae))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like handling categorical data with pipelines will require a separate \n",
    "# pipeline and FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cross-Validation<a class=\"anchor\" id=\"l2_part6\"></a>\n",
    "\n",
    "**Cross-Validation**\n",
    "* Use all training data for validation by splitting it into n folds and iteratively\n",
    "  withholding one fold for validation while training on the rest\n",
    "* Provides a more accurate measure of model quality, but take more time to run\n",
    "* Smaller sample sizes benefit more, three is no simple threshold for when to use it\n",
    "  + if the cross-validation scores have small standard deviation the benefit is minimal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 22620 +/- 840\n"
     ]
    }
   ],
   "source": [
    "# Simple cross-validation\n",
    "#   Using data and pipeline from Part5: pipelines (simple example)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "#print(scores)\n",
    "print('Mean Absolute Error {0:.0f} +/- {1:.0f}'.format(-1 * scores.mean(),\n",
    "                                                       scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 36330 +/- 1694\n"
     ]
    }
   ],
   "source": [
    "# Two feature cross-validation for comparison\n",
    "X2 = X.loc[:, ['LotArea', 'YearBuilt']]\n",
    "\n",
    "scores = cross_val_score(pipeline, X2, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "#print(scores)\n",
    "print('Mean Absolute Error {0:.0f} +/- {1:.0f}'.format(-1 * scores.mean(),\n",
    "                                                       scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 16097 +/- 1086\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation - All data model - w/ imputing and one hot encoding\n",
    "#   Combining postprocessing training and test sets, a bit of a hack since \n",
    "#   they never needed to be split, but let's keep things moving for now.\n",
    "#   \n",
    "X3 = pd.concat([X_train_final, X_test_final])\n",
    "y3 = pd.concat([y_train, y_test])\n",
    "\n",
    "scores = cross_val_score(pipeline, X3, y3, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Mean Absolute Error {0:.0f} +/- {1:.0f}'.format(-1 * scores.mean(),\n",
    "                                                       scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 Data Leakage:  <a class=\"anchor\" id=\"l2_part7\"></a>\n",
    "\n",
    "**Data Leakage**\n",
    "* When information from outside the training data is used to create the model  \n",
    "* Causes the model to look accurate until deployed\n",
    "\n",
    "\n",
    "**Leaky Predictors**\n",
    "* When the data used for model building (the train and test set data) contain information that will not be available when the model is used to make predictions in production.\n",
    "  + Can happen when the data contains a feature that is updated after the target is known \n",
    "    * i.e. a feature \"took_antibiotic\" when the target is \"got_pneumonia\"\n",
    "\n",
    "**Leaky Validation Strategies**\n",
    "* When validation data affects preprocessing steps\n",
    "  + If preprocessing occurs before a train_validation split \n",
    "  + When validation scores are used to select preprocessing methods, models and parameters without testing on an additional withheld data set\n",
    "\n",
    "**Preventing leakage**\n",
    "* Know your data\n",
    "* Exclude any features updated after the target is realized\n",
    "* Separate training data from validation data before preprocessing, even before data exploration\n",
    "* Withhold a separate test sets for model selection vs. selected model evaluation\n",
    "* Features that are highly correlated with the target are suspect\n",
    "* Any model that is more accurate than could be anticipated likely has a leakage problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath',\n",
       "       'BedroomAbvGr', 'TotRmsAbvGrd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review feature discriptions - none seem likely to be update after Sale \n",
    "# https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:        16097\n",
      "Avg price: 180921\n"
     ]
    }
   ],
   "source": [
    "# Quick - doesn't seem too good to be true\n",
    "print('MAE: {0:12.0f}'.format(-1 * scores.mean()))\n",
    "print('Avg price: {0:6.0f}'.format(np.average(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 358,
   "position": {
    "height": "40px",
    "left": "842px",
    "right": "21px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
